# æ•°æ®é¢„å¤„ç†


EasyAnimate å¯¹æ•°æ®è¿›è¡Œäº†åœºæ™¯åˆ‡åˆ†ã€è§†é¢‘è¿‡æ»¤å’Œè§†é¢‘æ‰“æ ‡æ¥å¾—åˆ°é«˜è´¨é‡çš„æœ‰æ ‡æ³¨è§†é¢‘è®­ç»ƒä½¿ç”¨ã€‚ä½¿ç”¨å¤šæ¨¡æ€å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)ä¸ºä»è§†é¢‘ä¸­æå–çš„å¸§ç”Ÿæˆå­—å¹•ï¼Œç„¶ååˆ©ç”¨LLMså°†ç”Ÿæˆçš„å¸§å­—å¹•æ€»ç»“å¹¶ç»†åŒ–ä¸ºæœ€ç»ˆçš„è§†é¢‘å­—å¹•ã€‚é€šè¿‡åˆ©ç”¨sglang/vLLMå’ŒåŠ é€Ÿåˆ†å¸ƒå¼æ¨ç†ï¼Œé«˜æ•ˆå®Œæˆè§†é¢‘çš„æ‰“æ ‡ã€‚

[English](./README.md) | ç®€ä½“ä¸­æ–‡

## å¿«é€Ÿå¼€å§‹
1. äº‘ä¸Šä½¿ç”¨: é˜¿é‡Œäº‘DSW/Docker
    å‚è€ƒ [README.md](../../README_zh-CN.md#quick-start) æŸ¥çœ‹æ›´å¤šç»†èŠ‚ã€‚

2. æœ¬åœ°å®‰è£…

    ```shell
    # Install EasyAnimate requirements firstly.
    cd EasyAnimate && pip install -r requirements.txt

    # Install additional requirements for video caption.
    cd easyanimate/video_caption && pip install -r requirements.txt --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/

    # Use DDP instead of DP in EasyOCR detection.
    site_pkg_path=$(python -c 'import site; print(site.getsitepackages()[0])')
    cp -v easyocr_detection_patched.py $site_pkg_path/easyocr/detection.py

    # We strongly recommend using Docker unless you can properly handle the dependency between vllm with torch(cuda).
    ```

## æ•°æ®é¢„å¤„ç†
æ•°æ®é¢„å¤„ç†å¯ä»¥åˆ†ä¸ºä¸€ä¸‹ä¸‰æ­¥ï¼š

- è§†é¢‘åˆ‡åˆ†
- è§†é¢‘è¿‡æ»¤
- è§†é¢‘æ‰“æ ‡

æ•°æ®é¢„å¤„ç†çš„è¾“å…¥å¯ä»¥æ˜¯è§†é¢‘æ–‡ä»¶å¤¹æˆ–åŒ…å«è§†é¢‘è·¯å¾„åˆ—çš„å…ƒæ•°æ®æ–‡ä»¶ï¼ˆtxt/csv/jsonlæ ¼å¼ï¼‰ã€‚è¯¦æƒ…è¯·æŸ¥çœ‹[utils/video_utils.py](utils/video_utils.py) æ–‡ä»¶ä¸­çš„ `get_video_path_list` å‡½æ•°ã€‚

ä¸ºäº†ä¾¿äºç†è§£ï¼Œæˆ‘ä»¬ä»¥Panda70mçš„ä¸€ä¸ªæ•°æ®ä¸ºä¾‹è¿›è¡Œæ•°æ®é¢„å¤„ç†ï¼Œç‚¹å‡»[è¿™é‡Œ](https://pai-aigc-photog.oss-cn-hangzhou.aliyuncs.com/easyanimate/asset/v2/--C66yU3LjM_2.mp4)ä¸‹è½½è§†é¢‘ã€‚è¯·ä¸‹è½½è§†é¢‘å¹¶æ”¾åœ¨ä¸‹é¢çš„è·¯å¾„ï¼š"datasets/panda_70m/before_vcut/"

```
ğŸ“¦ datasets/
â”œâ”€â”€ ğŸ“‚ panda_70m/
â”‚   â””â”€â”€ ğŸ“‚ before_vcut/
â”‚       â””â”€â”€ ğŸ“„ --C66yU3LjM_2.mp4
```

1. è§†é¢‘åˆ‡åˆ†

    å¯¹äºé•¿è§†é¢‘å‰ªè¾‘ï¼ŒEasyAnimate åˆ©ç”¨ PySceneDetect æ¥è¯†åˆ«è§†é¢‘ä¸­çš„åœºæ™¯å˜åŒ–ï¼Œå¹¶æ ¹æ®ç‰¹å®šçš„é˜ˆå€¼è¿›è¡Œåœºæ™¯åˆ‡å‰²ï¼Œä»¥ç¡®ä¿è§†é¢‘ç‰‡æ®µä¸»é¢˜çš„ä¸€è‡´æ€§ã€‚åˆ‡å‰²åï¼Œæˆ‘ä»¬åªä¿ç•™é•¿åº¦åœ¨3åˆ°10ç§’ä¹‹é—´çš„ç‰‡æ®µï¼Œç”¨äºæ¨¡å‹è®­ç»ƒã€‚

    æˆ‘ä»¬æ•´ç†äº†å®Œæ•´çš„æ–¹æ¡ˆåœ¨ ```stage_1_video_cut.sh``` æ–‡ä»¶ä¸­, æ‚¨å¯ä»¥ç›´æ¥è¿è¡Œ```stage_1_video_cut.sh```. æ‰§è¡Œå®Œæˆåå¯ä»¥åœ¨ ```easyanimate/video_caption/datasets/panda_70m/train``` æ–‡ä»¶å¤¹ä¸­æŸ¥çœ‹ç»“æœã€‚

    ```shell
    sh stage_1_video_cut.sh
    ```
2. è§†é¢‘è¿‡æ»¤

    éµå¾ªSVD([Stable Video Diffusion](https://github.com/Stability-AI/generative-models))çš„æ•°æ®å‡†å¤‡æµç¨‹ï¼ŒEasyAnimate æä¾›äº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æ•°æ®å¤„ç†ç®¡é“ï¼Œç”¨äºé«˜è´¨é‡æ•°æ®çš„è¿‡æ»¤å’Œæ ‡è®°ã€‚æˆ‘ä»¬è¿˜æ”¯æŒåˆ†å¸ƒå¼å¤„ç†æ¥åŠ å¿«æ•°æ®é¢„å¤„ç†çš„é€Ÿåº¦ã€‚æ•´ä¸ªè¿‡ç¨‹å¦‚ä¸‹ï¼š:

   - æ—¶é•¿è¿‡æ»¤: åˆ†æè§†é¢‘çš„åŸºæœ¬ä¿¡æ¯ï¼Œç­›é€‰å‡ºæ—¶é•¿è¿‡çŸ­æˆ–åˆ†è¾¨ç‡è¿‡ä½çš„ä½è´¨é‡è§†é¢‘ã€‚æˆ‘ä»¬ä¿ç•™3ç§’è‡³10ç§’çš„è§†é¢‘ã€‚
   - ç¾å­¦è¿‡æ»¤: é€šè¿‡è®¡ç®—å‡åŒ€åˆ†å¸ƒçš„4å¸§çš„å¹³å‡å®¡ç¾åˆ†æ•°ï¼Œè¿‡æ»¤æ‰å†…å®¹è´¨é‡å·®çš„è§†é¢‘ï¼ˆæ¨¡ç³Šã€æš—æ·¡ç­‰ï¼‰ã€‚
   - æ–‡æœ¬è¿‡æ»¤: ä½¿ç”¨ [easyocr](https://github.com/JaidedAI/EasyOCR) æ¥è®¡ç®—ä¸­é—´å¸§çš„æ–‡æœ¬æ¯”ä¾‹ï¼Œä»¥ç­›é€‰å‡ºå«æœ‰å¤§é‡æ–‡æœ¬çš„è§†é¢‘ã€‚
   - è¿åŠ¨è¿‡æ»¤: è®¡ç®—å¸§é—´å…‰æµå·®å¼‚ï¼Œä»¥ç­›é€‰å‡ºç§»åŠ¨è¿‡æ…¢æˆ–è¿‡å¿«çš„è§†é¢‘ã€‚

    **ç¾å­¦è¿‡æ»¤** çš„ä»£ç åœ¨ ```compute_video_frame_quality.py```. æ‰§è¡Œ ```compute_video_frame_quality.py```,æˆ‘ä»¬å¯ä»¥ç”Ÿæˆ ```datasets/panda_70m/aesthetic_score.jsonl```æ–‡ä»¶, è®¡ç®—æ¯æ¡è§†é¢‘çš„ç¾å­¦å¾—åˆ†ã€‚

    **æ–‡æœ¬è¿‡æ»¤** çš„ä»£ç åœ¨ ```compute_text_score.py```. æ‰§è¡Œ```compute_text_score.py```, æˆ‘ä»¬å¯ä»¥ç”Ÿæˆ ```datasets/panda_70m/text_score.jsonl```æ–‡ä»¶, è®¡ç®—æ¯ä¸ªè§†é¢‘çš„æ–‡å­—å æ¯”ã€‚

    **è¿åŠ¨è¿‡æ»¤** çš„ä»£ç åœ¨ ```compute_motion_score.py```. è¿åŠ¨è¿‡æ»¤åŸºäºå®¡ç¾è¿‡æ»¤å’Œæ–‡æœ¬è¿‡æ»¤ï¼›åªæœ‰è¾¾åˆ°ä¸€å®šå®¡ç¾åˆ†æ•°å’Œæ–‡æœ¬åˆ†æ•°çš„æ ·æœ¬æ‰ä¼šè¿›è¡Œè¿åŠ¨åˆ†æ•°çš„è®¡ç®—ã€‚ æ‰§è¡Œ ```compute_motion_score.py```, æˆ‘ä»¬å¯ä»¥ç”Ÿæˆ ```datasets/panda_70m/motion_score.jsonl```, è®¡ç®—æ¯æ¡è§†é¢‘çš„è¿åŠ¨å¾—åˆ†ã€‚

    æ¥ç€æ‰§è¡Œ ```filter_videos_by_motion_score.py```æ¥å¾—è¿‡æ»¤è§†é¢‘ã€‚æˆ‘ä»¬æœ€ç»ˆå¾—åˆ°ç­›é€‰åéœ€è¦æ‰“æ ‡çš„ ```datasets/panda_70m/train.jsonl```æ–‡ä»¶ã€‚

    æˆ‘ä»¬å°†è§†é¢‘è¿‡æ»¤çš„æµç¨‹æ•´ç†ä¸º ```stage_2_filter_data.sh```ï¼Œç›´æ¥æ‰§è¡Œè¯¥è„šæœ¬æ¥å®Œæˆè§†é¢‘æ•°æ®çš„è¿‡æ»¤ã€‚

    ```shell
    sh stage_2_filter_data.sh
    ```
3. è§†é¢‘æ‰“æ ‡

    
    è§†é¢‘æ‰“æ ‡ç”Ÿæˆåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µæ¶‰åŠä»è§†é¢‘ä¸­æå–å¸§å¹¶ä¸ºå®ƒä»¬ç”Ÿæˆæè¿°ã€‚éšåï¼Œä½¿ç”¨å¤§å‹è¯­è¨€æ¨¡å‹å°†è¿™äº›æè¿°æ±‡æ€»æˆä¸€æ¡å­—å¹•ã€‚

    æˆ‘ä»¬è¯¦ç»†å¯¹æ¯”äº†ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆè¯¸å¦‚[Qwen-VL](https://huggingface.co/Qwen/Qwen-VL), [ShareGPT4V-7B](https://huggingface.co/Lin-Chen/ShareGPT4V-7B), [deepseek-vl-7b-chat](https://huggingface.co/deepseek-ai/deepseek-vl-7b-chat)ï¼‰ç”Ÿæˆæ–‡æœ¬æè¿°çš„æ•ˆæœã€‚ æœ€ç»ˆé€‰æ‹© [llava-v1.6-vicuna-7b](https://huggingface.co/liuhaotian/llava-v1.6-vicuna-7b) æ¥è¿›è¡Œè§†é¢‘æ–‡æœ¬æè¿°çš„ç”Ÿæˆï¼Œå®ƒèƒ½ç”Ÿæˆè¯¦ç»†çš„æè¿°å¹¶æœ‰æ›´å°‘çš„å¹»è§‰ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥ [sglang](https://github.com/sgl-project/sglang)ï¼Œ[lmdepoly](https://github.com/InternLM/lmdeploy), æ¥åŠ é€Ÿæ¨ç†çš„è¿‡ç¨‹ã€‚

    é¦–å…ˆï¼Œæˆ‘ä»¬ç”¨ ```caption_video_frame.py``` æ¥ç”Ÿæˆæ–‡æœ¬æè¿°ï¼Œå¹¶ç”¨ ```caption_summary.py``` æ¥æ€»ç»“æè¿°ä¿¡æ¯ã€‚æˆ‘ä»¬å°†ä¸Šè¿°è¿‡ç¨‹æ•´ç†åœ¨ ```stage_3_video_caption.sh```, ç›´æ¥è¿è¡Œå®ƒæ¥ç”Ÿæˆè§†é¢‘çš„æ–‡æœ¬æè¿°ã€‚æˆ‘ä»¬æœ€ç»ˆå¾—åˆ° ```train_panda_70m.json``` ç”¨äºEasyAnmate çš„è®­ç»ƒã€‚ 

    ```shell
    sh stage_3_video_caption.sh
    ```

   è¯·æ³¨æ„ï¼Œå¦‚é‡ç½‘ç»œé—®é¢˜ï¼Œæ‚¨å¯ä»¥è®¾ç½® `export HF_ENDPOINT=https://hf-mirror.com` æ¥è‡ªåŠ¨ä¸‹è½½è§†é¢‘æ‰“æ ‡æ¨¡å‹ã€‚