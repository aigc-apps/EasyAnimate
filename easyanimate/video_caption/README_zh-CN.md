# æ•°æ®é¢„å¤„ç†
[English](./README.md) | ç®€ä½“ä¸­æ–‡

è¯¥æ–‡ä»¶å¤¹åŒ…å« EasyAnimate ä½¿ç”¨çš„æ•°æ®é›†é¢„å¤„ç†ï¼ˆå³è§†é¢‘åˆ‡åˆ†ï¼‰ã€è¿‡æ»¤å’Œç”Ÿæˆæè¿°çš„ä»£ç ã€‚æ•´ä¸ªè¿‡ç¨‹æ”¯æŒåˆ†å¸ƒå¼å¹¶è¡Œå¤„ç†ï¼Œèƒ½å¤Ÿå¤„ç†å¤§è§„æ¨¡æ•°æ®é›†ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬å’Œ [Data-Juicer](https://github.com/modelscope/data-juicer/blob/main/docs/DJ_SORA.md) åˆä½œï¼Œèƒ½è®©ä½ åœ¨ [Aliyun PAI-DLC](https://help.aliyun.com/zh/pai/user-guide/video-preprocessing/) è½»æ¾è¿›è¡Œè§†é¢‘æ•°æ®çš„å¤„ç†ã€‚


## å¿«é€Ÿå¼€å§‹

### å®‰è£…
æ¨èä½¿ç”¨é˜¿é‡Œäº‘ DSW å’Œ Docker æ¥å®‰è£…ç¯å¢ƒï¼Œè¯·å‚è€ƒ [å¿«é€Ÿå¼€å§‹](../../README_zh-CN.md#1-äº‘ä½¿ç”¨-aliyundswdocker). ä½ ä¹Ÿå¯ä»¥å‚è€ƒ [Dockerfile](../../Dockerfile.ds) ä¸­çš„é•œåƒæ„å»ºæµç¨‹åœ¨æœ¬åœ°å®‰è£…å¯¹åº”çš„ conda ç¯å¢ƒå’Œå…¶ä½™ä¾èµ–ã€‚

ä¸ºäº†æé«˜æ¨ç†é€Ÿåº¦å’ŒèŠ‚çœæ¨ç†çš„æ˜¾å­˜ï¼Œç”Ÿæˆè§†é¢‘æè¿°ä¾èµ–äº [llm-awq](https://github.com/mit-han-lab/llm-awq)ã€‚å› æ­¤ï¼Œéœ€è¦ RTX 3060 æˆ–è€… A2 åŠä»¥ä¸Šçš„æ˜¾å¡ (CUDA Compute Capability >= 8.0)ã€‚

```shell
# pull image
docker pull mybigpai-public-registry.cn-beijing.cr.aliyuncs.com/easycv/torch_cuda:
asyanimate

# enter image
docker run -it -p 7860:7860 --network host --gpus all --security-opt seccomp:unconfined --shm-size 200g mybigpai-public-registry.cn-beijing.cr.aliyuncs.com/easycv/torch_cuda:asyanimate

# clone code
git clone https://github.com/aigc-apps/EasyAnimate.git

# enter video_caption
cd EasyAnimate/easyanimate/video_caption
```

### æ•°æ®å‡†å¤‡
å°†ä¸‹è½½çš„è§†é¢‘å‡†å¤‡åˆ°æ–‡ä»¶å¤¹ [datasets](./datasets/)ï¼ˆæœ€å¥½ä¸ä½¿ç”¨åµŒå¥—ç»“æ„ï¼Œå› ä¸ºè§†é¢‘åç§°åœ¨åç»­å¤„ç†ä¸­ç”¨ä½œå”¯ä¸€ IDï¼‰ã€‚ä»¥ Panda-70M ä¸ºä¾‹ï¼Œå®Œæ•´çš„æ•°æ®é›†ç›®å½•ç»“æ„å¦‚ä¸‹æ‰€ç¤ºï¼š
```
ğŸ“¦ datasets/
â”œâ”€â”€ ğŸ“‚ panda_70m/
â”‚   â”œâ”€â”€ ğŸ“‚ videos/
â”‚   â”‚   â”œâ”€â”€ ğŸ“‚ data/
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ --C66yU3LjM_2.mp4
â”‚   â”‚   â”‚   â””â”€â”€ ğŸ“„ ...
```

### è§†é¢‘åˆ‡åˆ†
EasyAnimate ä½¿ç”¨ [PySceneDetect](https://github.com/Breakthrough/PySceneDetect) æ¥è¯†åˆ«è§†é¢‘ä¸­çš„åœºæ™¯å˜åŒ–
å¹¶æ ¹æ®æŸäº›é˜ˆå€¼é€šè¿‡ FFmpeg æ‰§è¡Œè§†é¢‘åˆ†å‰²ï¼Œä»¥ç¡®ä¿è§†é¢‘ç‰‡æ®µçš„ä¸€è‡´æ€§ã€‚
çŸ­äº 3 ç§’çš„è§†é¢‘ç‰‡æ®µå°†è¢«ä¸¢å¼ƒï¼Œé•¿äº 10 ç§’çš„è§†é¢‘ç‰‡æ®µå°†è¢«é€’å½’åˆ‡åˆ†ã€‚

è§†é¢‘åˆ‡åˆ†çš„å®Œæ•´æµç¨‹åœ¨ [stage_1_video_splitting.sh](./scripts/stage_1_video_splitting.sh)ã€‚æ‰§è¡Œ
```shell
sh scripts/stage_1_video_splitting.sh
```
åï¼Œåˆ‡åˆ†åçš„è§†é¢‘ä½äº `easyanimate/video_caption/datasets/panda_70m/videos_clips/data/`ã€‚

### è§†é¢‘è¿‡æ»¤
åŸºäºä¸Šä¸€æ­¥è·å¾—çš„è§†é¢‘ï¼ŒEasyAnimate æä¾›äº†ä¸€ä¸ªç®€å•è€Œæœ‰æ•ˆçš„æµç¨‹æ¥è¿‡æ»¤å‡ºé«˜è´¨é‡çš„è§†é¢‘ã€‚æ€»ä½“æµç¨‹å¦‚ä¸‹ï¼š

- ç¾å­¦è¿‡æ»¤ï¼šé€šè¿‡ [aesthetic-predictor-v2-5](https://github.com/discus0434/aesthetic-predictor-v2-5) è®¡ç®—å‡åŒ€é‡‡æ ·çš„ 4 å¸§è§†é¢‘çš„å¹³å‡ç¾å­¦åˆ†æ•°ï¼Œä»è€Œç­›é€‰å‡ºå†…å®¹ä¸ä½³ï¼ˆæ¨¡ç³Šã€æ˜æš—ç­‰ï¼‰çš„è§†é¢‘ã€‚
- æ–‡æœ¬è¿‡æ»¤ï¼šä½¿ç”¨ [EasyOCR](https://github.com/JaidedAI/EasyOCR) è®¡ç®—ä¸­é—´å¸§çš„æ–‡æœ¬åŒºåŸŸæ¯”ä¾‹ï¼Œè¿‡æ»¤æ‰å«æœ‰å¤§é¢ç§¯æ–‡æœ¬çš„è§†é¢‘ã€‚
- è¿åŠ¨è¿‡æ»¤ï¼šè®¡ç®—å¸§é—´å…‰æµå·®ï¼Œè¿‡æ»¤æ‰ç§»åŠ¨å¤ªæ…¢æˆ–å¤ªå¿«çš„è§†é¢‘ã€‚

è§†é¢‘è¿‡æ»¤çš„å®Œæ•´æµç¨‹åœ¨ [stage_2_video_filtering.sh](./scripts/stage_2_video_filtering.sh)ã€‚æ‰§è¡Œ
```shell
sh scripts/stage_2_video_filtering.sh
```
åï¼Œè§†é¢‘çš„ç¾å­¦å¾—åˆ†ã€æ–‡æœ¬å¾—åˆ†å’Œè¿åŠ¨å¾—åˆ†å¯¹åº”çš„å…ƒæ–‡ä»¶ä¿å­˜åœ¨ `easyanimate/video_caption/datasets/panda_70m/videos_clips/`ã€‚

> [!NOTE]
> ç¾å­¦å¾—åˆ†çš„è®¡ç®—ä¾èµ–äº [google/siglip-so400m-patch14-384 model](https://huggingface.co/google/siglip-so400m-patch14-384).
è¯·æ‰§è¡Œ `HF_ENDPOINT=https://hf-mirror.com sh scripts/stage_2_video_filtering.sh` å¦‚æœä½ æ— æ³•è®¿é—® huggingface.com.

### è§†é¢‘æè¿°
åœ¨è·å¾—ä¸Šè¿°é«˜è´¨é‡çš„è¿‡æ»¤è§†é¢‘åï¼ŒEasyAnimate åˆ©ç”¨ [VILA1.5](https://github.com/NVlabs/VILA) æ¥ç”Ÿæˆè§†é¢‘æè¿°ã€‚éšåï¼Œä½¿ç”¨ LLMs å¯¹ç”Ÿæˆçš„è§†é¢‘æè¿°è¿›è¡Œé‡å†™ï¼Œä»¥æ›´å¥½åœ°æ»¡è¶³è§†é¢‘ç”Ÿæˆä»»åŠ¡çš„è¦æ±‚ã€‚æœ€åï¼Œä½¿ç”¨è‡ªç ”çš„ VideoCLIPXL æ¨¡å‹æ¥è¿‡æ»¤æ‰æè¿°å’Œè§†é¢‘å†…å®¹ä¸ä¸€è‡´çš„æ•°æ®ï¼Œä»è€Œå¾—åˆ°æœ€ç»ˆçš„è®­ç»ƒæ•°æ®é›†ã€‚

è¯·æ ¹æ®æœºå™¨çš„æ˜¾å­˜ä» [VILA1.5](https://huggingface.co/collections/Efficient-Large-Model/vila-on-pre-training-for-visual-language-models-65d8022a3a52cd9bcd62698e) ä¸‹è½½åˆé€‚å¤§å°çš„æ¨¡å‹ã€‚å¯¹äº A100 40Gï¼Œä½ å¯ä»¥æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤æ¥ä¸‹è½½ [VILA1.5-40b-AWQ](https://huggingface.co/Efficient-Large-Model/VILA1.5-40b-AWQ)
```shell
# Add HF_ENDPOINT=https://hf-mirror.com before the command if you cannot access to huggingface.com
huggingface-cli download Efficient-Large-Model/VILA1.5-40b-AWQ --local-dir-use-symlinks False --local-dir /PATH/TO/VILA_MODEL
```

ä½ å¯ä»¥é€‰æ‹©æ€§åœ°å‡†å¤‡ LLMs æ¥æ”¹å†™ä¸Šè¿°è§†é¢‘æè¿°çš„ç»“æœã€‚ä¾‹å¦‚ï¼Œä½ æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤æ¥ä¸‹è½½ [Meta-Llama-3-8B-Instruct](https://huggingface.co/NousResearch/Meta-Llama-3-8B-Instruct)
```shell
# Add HF_ENDPOINT=https://hf-mirror.com before the command if you cannot access to huggingface.com
huggingface-cli download NousResearch/Meta-Llama-3-8B-Instruct --local-dir-use-symlinks False --local-dir /PATH/TO/REWRITE_MODEL
```

è§†é¢‘æè¿°çš„å®Œæ•´æµç¨‹åœ¨ [stage_3_video_recaptioning.sh](./scripts/stage_3_video_recaptioning.sh).
æ‰§è¡Œ
```shell
VILA_MODEL_PATH=/PATH/TO/VILA_MODEL REWRITE_MODEL_PATH=/PATH/TO/REWRITE_MODEL sh scripts/stage_3_video_recaptioning.sh
```
åï¼Œæœ€åçš„è®­ç»ƒæ–‡ä»¶ä¼šä¿å­˜åœ¨ `easyanimate/video_caption/datasets/panda_70m/videos_clips/meta_train_info.json`ã€‚